from langchain_community.llms import Ollama
import requests
import json

llm = Ollama(model="llama3")

# Configuraci√≥n de modelos disponibles
MODELS_CONFIG = {
    "bitcoin": {
        "endpoint": "http://localhost:8000/bitcoin/models/bitcoin/predict",
        "description": "Para predicciones de precios de Bitcoin, criptomonedas, an√°lisis financiero",
        "available": True,
        "response_type": "prediction"
    },
    "properties": {
        "endpoint": "http://localhost:8000/properties/models/properties/predict",
        "description": "Para predicci√≥n de precios de propiedades inmobiliarias, casas, apartamentos",
        "available": True,
        "response_type": "prediction"
    },
    "movies": {
        "endpoint": "http://localhost:8000/movies/models/movies/recommend",
        "description": "Para recomendaciones de pel√≠culas personalizadas basadas en preferencias",
        "available": True,
        "response_type": "recommendation"
    },
    "flights": {
        "endpoint": "http://localhost:8000/flights/models/flights/predict",
        "description": "Para predicciones de retrasos de vuelos, an√°lisis de puntualidad y planificaci√≥n de viajes",
        "available": True,
        "response_type": "flight_prediction"
    },
    "wine": {
        "endpoint": "http://localhost:8000/wine/classify",
        "description": "Para clasificaci√≥n de vinos basada en caracter√≠sticas qu√≠micas",
        "available": False,
        "response_type": "classification"
    },
    "churn": {
        "endpoint": "http://localhost:8000/churn/predict",
        "description": "Para predicci√≥n de abandono de clientes",
        "available": False,
        "response_type": "prediction"
    },
    "emotions": {
        "endpoint": "http://localhost:8000/emotions/analyze",
        "description": "Para an√°lisis de emociones en texto",
        "available": False,
        "response_type": "classification"
    }
}

def extract_bitcoin_parameters(query: str):
    """
    Extrae par√°metros num√©ricos del texto para el modelo Bitcoin
    """
    extraction_prompt = f"""
    Extrae valores num√©ricos espec√≠ficos para predicci√≥n de Bitcoin del siguiente texto:
    
    "{query}"
    
    Busca y extrae SOLO los valores que se mencionen expl√≠citamente:
    - Precio actual/open (ej: "precio actual 32500", "bitcoin est√° en 31000")
    - Precio m√°ximo/high (ej: "m√°ximo 33000", "high 32800")
    - Precio m√≠nimo/low (ej: "m√≠nimo 31500", "low 31200")
    - Volumen (ej: "volumen 2B", "2 billones de volumen", "1.5B USD")
    - Market cap (ej: "market cap 600B", "capitalizaci√≥n 700 billones")
    - RSI (ej: "RSI 65", "RSI de 72.5")
    - Medias m√≥viles (ej: "MA5 31800", "media m√≥vil 20 d√≠as 31500")
    
    Responde SOLO en formato JSON v√°lido con los valores encontrados:
    {{
        "open_price": 32500.0,
        "high_price": null,
        "volume": 2000000000.0,
        "rsi_14": 65.0
    }}
    
    Si NO encuentras un valor espec√≠fico, usa null.
    NO inventes valores, SOLO extrae los mencionados expl√≠citamente.
    """
    
    try:
        extraction_result = llm.invoke(extraction_prompt)
        # Intentar parsear como JSON
        import json
        import re
        
        # Limpiar la respuesta para extraer solo el JSON
        json_match = re.search(r'\{.*\}', extraction_result, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            extracted_params = json.loads(json_str)
            # Filtrar valores null
            filtered_params = {k: v for k, v in extracted_params.items() if v is not None}
            return filtered_params
        else:
            return {}
    except Exception as e:
        print(f"Error extrayendo par√°metros: {e}")
        return {}

def extract_properties_parameters(query: str):
    """
    Extrae par√°metros para predicci√≥n de precios de propiedades
    """
    extraction_prompt = f"""
    Extrae caracter√≠sticas de propiedades del siguiente texto:
    
    "{query}"
    
    Busca y extrae SOLO los valores mencionados expl√≠citamente:
    - Ba√±os (ej: "3 ba√±os", "2.5 bathrooms", "4 bath")
    - Habitaciones (ej: "4 habitaciones", "3 bedrooms", "5 bed")
    - Pies cuadrados (ej: "2500 sq ft", "1800 pies cuadrados", "3000 square feet")
    - A√±o construcci√≥n (ej: "construida en 1990", "built in 2005", "a√±o 2010")
    - Tama√±o del lote (ej: "7000 sq ft lot", "0.5 acres", "5000 pies cuadrados de terreno")
    - Coordenadas (ej: "latitud 34.05", "longitude -118.25")
    - Impuestos (ej: "taxes $5000", "impuestos 4500 anuales")
    
    Responde SOLO en formato JSON v√°lido:
    {{
        "bathroomcnt": 3.0,
        "bedroomcnt": 4.0,
        "finishedsquarefeet": 2500.0,
        "yearbuilt": 1990.0,
        "lotsizesquarefeet": 7000.0,
        "latitude": null,
        "longitude": null,
        "taxamount": 5000.0
    }}
    
    Si NO encuentras un valor espec√≠fico, usa null.
    """
    
    try:
        extraction_result = llm.invoke(extraction_prompt)
        import json
        import re
        
        json_match = re.search(r'\{.*\}', extraction_result, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            extracted_params = json.loads(json_str)
            filtered_params = {k: v for k, v in extracted_params.items() if v is not None}
            return filtered_params
        else:
            return {}
    except Exception as e:
        print(f"Error extrayendo par√°metros de propiedades: {e}")
        return {}

def extract_flights_parameters(query: str):
    """
    Extrae par√°metros para predicci√≥n de retrasos de vuelos
    """
    extraction_prompt = f"""
    Extrae informaci√≥n de vuelos del siguiente texto:
    
    "{query}"
    
    Busca y extrae SOLO los valores mencionados expl√≠citamente:
    - Fecha de vuelo (ej: "ma√±ana", "25 de octubre", "2025-10-25", "hoy")
    - Hora de salida (ej: "7:00 AM", "19:30", "3 p.m.", "15:00")
    - Aeropuerto origen (ej: "SFO", "San Francisco", "LAX", "Los Angeles", "Denver", "Las Vegas")
    - Aeropuerto destino (ej: "JFK", "Nueva York", "ORD", "Chicago")
    - Aerol√≠nea (ej: "United", "UA", "American Airlines", "AA", "Delta", "DL", "Southwest", "WN")
    - Distancia (ej: "2586 km", "1500 millas") - SOLO si se menciona expl√≠citamente
    - Retraso en salida (ej: "retraso de 15 minutos", "sale con 20 min de atraso") - SOLO si se menciona expl√≠citamente
    
    INSTRUCCIONES IMPORTANTES:
    - Convierte c√≥digos de aeropuertos a c√≥digos IATA de 3 letras
    - Convierte fechas relativas a formato YYYY-MM-DD (hoy es 2025-10-24)
    - Convierte horas a formato HH:MM (24 horas)
    - Si NO encuentras un valor espec√≠fico, NO lo incluyas en la respuesta
    - Para delay_at_departure usa SOLO n√∫meros (ej: 15, 0, 30), NUNCA texto
    
    Mapeo de aerol√≠neas:
    - Southwest = WN
    - United = UA  
    - American = AA
    - Delta = DL
    - JetBlue = B6
    
    Mapeo de aeropuertos:
    - Denver = DEN
    - Las Vegas = LAS
    - San Francisco = SFO
    - New York JFK = JFK
    - Los Angeles = LAX
    - Chicago = ORD
    
    Responde SOLO en formato JSON v√°lido:
    {{
        "date": "2025-10-24",
        "departure_time": "15:00",
        "origin": "DEN",
        "destination": "LAS",
        "airline": "WN"
    }}
    
    NO incluyas campos con valores null, undefined, o texto descriptivo.
    Si no hay retraso mencionado, NO incluyas delay_at_departure.
    """
    
    try:
        extraction_result = llm.invoke(extraction_prompt)
        import json
        import re
        from datetime import datetime, timedelta
        
        json_match = re.search(r'\{.*\}', extraction_result, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            extracted_params = json.loads(json_str)
            
            # Procesar fecha si es relativa
            if extracted_params.get("date"):
                date_str = extracted_params["date"]
                if "ma√±ana" in date_str.lower() or "tomorrow" in date_str.lower():
                    tomorrow = datetime.now() + timedelta(days=1)
                    extracted_params["date"] = tomorrow.strftime("%Y-%m-%d")
                elif "hoy" in date_str.lower() or "today" in date_str.lower():
                    today = datetime.now()
                    extracted_params["date"] = today.strftime("%Y-%m-%d")
            
            # Validar y limpiar valores num√©ricos
            if "delay_at_departure" in extracted_params:
                delay_value = extracted_params["delay_at_departure"]
                if isinstance(delay_value, str):
                    # Intentar extraer n√∫meros del texto
                    import re
                    numbers = re.findall(r'\d+', delay_value)
                    if numbers:
                        extracted_params["delay_at_departure"] = float(numbers[0])
                    else:
                        # Si no hay n√∫meros, remover el campo
                        del extracted_params["delay_at_departure"]
                elif not isinstance(delay_value, (int, float)):
                    del extracted_params["delay_at_departure"]
            
            if "distance" in extracted_params:
                distance_value = extracted_params["distance"]
                if isinstance(distance_value, str):
                    # Intentar extraer n√∫meros del texto
                    import re
                    numbers = re.findall(r'\d+', distance_value)
                    if numbers:
                        extracted_params["distance"] = float(numbers[0])
                    else:
                        del extracted_params["distance"]
                elif not isinstance(distance_value, (int, float)):
                    del extracted_params["distance"]
            
            # Filtrar valores null y vac√≠os
            filtered_params = {k: v for k, v in extracted_params.items() 
                             if v is not None and v != "" and v != "null"}
            return filtered_params
        else:
            return {}
    except Exception as e:
        print(f"Error extrayendo par√°metros de vuelos: {e}")
        return {}

def extract_movies_parameters(query: str):
    """
    Extrae par√°metros para recomendaciones de pel√≠culas
    """
    extraction_prompt = f"""
    Extrae informaci√≥n para recomendaciones de pel√≠culas del siguiente texto:
    
    "{query}"
    
    Busca y extrae SOLO los valores mencionados expl√≠citamente:
    - ID de pel√≠cula (ej: "pel√≠cula ID 5", "movie 10", "film 25")
    - ID de usuario (ej: "usuario 15", "user 8", "mi ID es 20")
    - T√≠tulo de pel√≠cula (ej: "Toy Story", "Jumanji", "Heat")
    - G√©nero (ej: "acci√≥n", "comedia", "drama", "thriller")
    - N√∫mero de recomendaciones (ej: "5 pel√≠culas", "recomienda 3", "top 10")
    
    Responde SOLO en formato JSON v√°lido:
    {{
        "movie_id": 5,
        "user_id": 15,
        "movie_title": "Toy Story",
        "genre": "acci√≥n",
        "num_recommendations": 5
    }}
    
    Si NO encuentras un valor espec√≠fico, usa null.
    """
    
    try:
        extraction_result = llm.invoke(extraction_prompt)
        import json
        import re
        
        json_match = re.search(r'\{.*\}', extraction_result, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            extracted_params = json.loads(json_str)
            filtered_params = {k: v for k, v in extracted_params.items() if v is not None}
            return filtered_params
        else:
            return {}
    except Exception as e:
        print(f"Error extrayendo par√°metros de pel√≠culas: {e}")
        return {}

def get_available_models():
    """Retorna lista de modelos disponibles"""
    return {name: config for name, config in MODELS_CONFIG.items() if config["available"]}

def interpretar_y_ejecutar(query: str):
    """
    Coordinador principal que decide qu√© modelo usar y ejecuta la consulta
    """
    # Paso 1: el LLM decide qu√© modelo usar
    available_models = get_available_models()
    
    # Construir la descripci√≥n de modelos disponibles din√°micamente
    models_description = "\n".join([
        f"    - {name}: {config['description']}"
        for name, config in MODELS_CONFIG.items()
        if config["available"]
    ])
    
    # Agregar modelos no disponibles
    unavailable_models = "\n".join([
        f"    - {name}: {config['description']} (no disponible a√∫n)"
        for name, config in MODELS_CONFIG.items()
        if not config["available"]
    ])
    
    decision_prompt = f"""
    Eres un coordinador de modelos de IA. Analiza la siguiente consulta y decide qu√© modelo usar.

    Consulta: "{query}"

    Modelos disponibles:
{models_description}

    Modelos en desarrollo:
{unavailable_models}

    Responde SOLO con el nombre del modelo m√°s apropiado ({', '.join(MODELS_CONFIG.keys())}).
    Si no hay un modelo apropiado, responde "ninguno".
    """
    
    decision = llm.invoke(decision_prompt)
    modelo = decision.strip().lower()

    # Paso 2: verificar si el modelo est√° disponible y hacer la consulta
    if modelo in MODELS_CONFIG:
        model_config = MODELS_CONFIG[modelo]
        
        if not model_config["available"]:
            return f"El modelo '{modelo}' est√° en desarrollo y no est√° disponible a√∫n. Actualmente solo tengo disponible: {', '.join(get_available_models().keys())}"
        
        # Hacer la consulta al modelo
        try:
            data = {"query": query}
            
            # Extraer par√°metros espec√≠ficos seg√∫n el modelo
            if modelo == "bitcoin":
                bitcoin_params = extract_bitcoin_parameters(query)
                if bitcoin_params:
                    data.update(bitcoin_params)
                    print(f"üéØ Par√°metros extra√≠dos para Bitcoin: {bitcoin_params}")
            
            elif modelo == "flights":
                flights_params = extract_flights_parameters(query)
                if flights_params:
                    data.update(flights_params)
                    print(f"‚úàÔ∏è Par√°metros extra√≠dos para Vuelos: {flights_params}")
            
            elif modelo == "properties":
                properties_params = extract_properties_parameters(query)
                if properties_params:
                    data.update(properties_params)
                    print(f"üè† Par√°metros extra√≠dos para Propiedades: {properties_params}")
            
            elif modelo == "movies":
                movies_params = extract_movies_parameters(query)
                if movies_params:
                    data.update(movies_params)
                    print(f"üé¨ Par√°metros extra√≠dos para Pel√≠culas: {movies_params}")
                
                # Para pel√≠culas, podr√≠amos necesitar un endpoint diferente si es predicci√≥n de rating
                if "user_id" in data and "movie_id" in data:
                    model_config["endpoint"] = "http://localhost:8000/movies/models/movies/predict-rating"
            
            response = requests.post(model_config["endpoint"], json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
            else:
                return f"Error al consultar el modelo {modelo}: {response.status_code} - {response.text}"
                
        except requests.exceptions.RequestException as e:
            return f"Error de conexi√≥n con el modelo {modelo}: {str(e)}"
        except Exception as e:
            return f"Error inesperado al consultar {modelo}: {str(e)}"
    else:
        if modelo == "ninguno":
            available_list = ', '.join(get_available_models().keys())
            return f"Lo siento, no tengo un modelo espec√≠fico para responder a esa consulta. Actualmente puedo ayudarte con: {available_list}"
        else:
            return f"El modelo '{modelo}' no existe. Modelos disponibles: {', '.join(get_available_models().keys())}"

    # Paso 3: interpreta el resultado con el LLM
    interpretation_prompt = f"""
    Un modelo de {modelo} (tipo: {model_config['response_type']}) devolvi√≥ este resultado para la consulta "{query}":

    Resultado: {json.dumps(result, indent=2)}

    Tu tarea es interpretar este resultado y explic√°rselo al usuario de forma natural, clara y √∫til.

    Instrucciones espec√≠ficas seg√∫n el tipo de modelo:
    - Si es 'time_series_prediction' (predicci√≥n temporal): Explica las tendencias, fechas espec√≠ficas, valores predichos y intervalos de confianza
    - Si es 'flight_prediction' (predicci√≥n de vuelos): Explica el retraso esperado, factores que influyen, nivel de confianza y recomendaciones
    - Si es 'prediction' (predicci√≥n): Incluye el valor predicho, tendencia y nivel de confianza
    - Si es 'classification' (clasificaci√≥n): Explica la categor√≠a predicha y probabilidad
    - Si es 'recommendation' (recomendaci√≥n): Lista las recomendaciones principales y razones

    Instrucciones generales:
    1. Explica qu√© significa el resultado en t√©rminos simples
    2. Menciona cualquier limitaci√≥n o consideraci√≥n importante
    3. S√© conciso pero informativo
    4. Usa emojis apropiados para hacer la respuesta m√°s amigable

    Respuesta:
    """

    try:
        # Siempre usar el LLM para generar una respuesta conversacional completa
        explicacion = llm.invoke(interpretation_prompt)
        return explicacion
    except Exception as e:
        # Si falla la interpretaci√≥n, devolver el resultado de forma m√°s amigable
        return format_fallback_response(modelo, result, model_config['response_type'])

def format_fallback_response(modelo: str, result: dict, response_type: str):
    """
    Formatea una respuesta de respaldo cuando falla la interpretaci√≥n del LLM
    """
    try:
        if response_type == "flight_prediction" and modelo == "flights":
            # Nuevo formato para predicci√≥n de vuelos
            if "prediction" in result:
                delay_minutes = result.get("prediction", 0)
                confidence = result.get("confidence", 0)
                flight_info = result.get("flight_info", {})
                
                route = flight_info.get("route", "Vuelo")
                airline = flight_info.get("airline", "")
                departure = flight_info.get("departure", "")
                
                if delay_minutes <= 5:
                    status_emoji = "‚úÖ"
                    status = "puntual"
                elif delay_minutes <= 15:
                    status_emoji = "üü°"
                    status = "retraso leve"
                elif delay_minutes <= 30:
                    status_emoji = "üü†"
                    status = "retraso moderado"
                else:
                    status_emoji = "üî¥"
                    status = "retraso significativo"
                
                response = f"{status_emoji} Predicci√≥n vuelo {airline} {route}:\n"
                response += f"üïê Retraso esperado: {delay_minutes:.0f} minutos ({status})\n"
                response += f"üìÖ Salida: {departure}\n"
                response += f"üéØ Confianza: {confidence:.1f}%"
                return response
                
        elif response_type == "time_series_prediction" and modelo == "bitcoin":
            # Nuevo formato para el modelo Prophet de Bitcoin
            if "predictions" in result:
                predictions = result.get("predictions", [])
                if predictions:
                    # Mostrar las primeras 3 predicciones
                    preview = predictions[:3]
                    formatted_preds = []
                    for pred in preview:
                        date = pred.get("date", "Fecha desconocida")
                        price = pred.get("predicted_price", 0)
                        formatted_preds.append(f"{date}: ${price:,.2f}")
                    
                    total_days = len(predictions)
                    confidence = result.get("confidence", 0)
                    model_type = result.get("model_info", {}).get("model_type", "Prophet")
                    
                    response = f"üìà Predicciones Bitcoin ({model_type}):\n"
                    response += "\n".join(formatted_preds)
                    if total_days > 3:
                        response += f"\n... y {total_days - 3} d√≠as m√°s"
                    response += f"\n\nüéØ Confianza del modelo: {confidence:.1f}%"
                    return response
            
            elif "prediction" in result:
                # Formato de respaldo para predicci√≥n √∫nica
                prediction = result.get("prediction", 0)
                confidence = result.get("confidence", 0)
                return f"üí∞ Predicci√≥n de Bitcoin: ${prediction:,.2f} USD (Confianza: {confidence:.1f}%)"
            
            elif modelo == "properties" and "prediction" in result:
                prediction = result.get("prediction", 0)
                confidence = result.get("confidence", 0)
                return f"üè† Precio estimado de propiedad: ${prediction:,.2f} USD (Confianza: {confidence:.1f}%)"
            
            # TODO: Agregar formato para otros modelos de predicci√≥n (churn, etc.)
            
        elif response_type == "classification":
            # TODO: Implementar formato para modelos de clasificaci√≥n (wine, emotions)
            if "predicted_class" in result:
                predicted_class = result.get("predicted_class", "Desconocido")
                probability = result.get("probability", 0)
                return f"üéØ Clasificaci√≥n: {predicted_class} (Probabilidad: {probability:.1f}%)"
                
        elif response_type == "recommendation":
            if modelo == "movies":
                if "recommendations" in result:
                    recs = result.get("recommendations", [])[:3]  # Top 3
                    if recs:
                        movie_titles = [rec.get("title", "Pel√≠cula desconocida") for rec in recs]
                        return f"üé¨ Recomendaciones de pel√≠culas: {', '.join(movie_titles)}"
                
                elif "predicted_rating" in result:
                    rating = result.get("predicted_rating", 0)
                    confidence = result.get("confidence", 0)
                    movie_title = result.get("model_info", {}).get("movie_title", "Pel√≠cula")
                    return f"üé¨ Rating predicho para {movie_title}: {rating:.1f}/5.0 (Confianza: {confidence:.1f}%)"
        
        # Respuesta gen√©rica si no hay formato espec√≠fico
        return f"Resultado del modelo {modelo}: {json.dumps(result, indent=2)}"
        
    except Exception:
        return f"Resultado del modelo {modelo}: {result}"
